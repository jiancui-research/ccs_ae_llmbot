# **The Odyssey of robots.txt Governance: Measuring Compliance Implications of Web Crawling Bots in Large Language Model Services**  

This repository contains the implementation details of our paper titled:  
*"The Odyssey of robots.txt Governance: Measuring Compliance Implications of Web Crawling Bots in Large Language Model Services."*  

## 📋 **Artifact Overview**

This artifact validates the following claims from our paper:
- **Section 4.2**: Web Publishers' Response to LLM Bots - Awareness and adoption patterns
- **Section 4.2**: Web Publishers' preferences on LLM bots  
- **Section 5.1**: Memorization analysis for open-source and closed-source LLMs
- **Section 5.2**: Case Study: ChatGPT-User behavior analysis

**Total estimated time**: 45-60 human-minutes

## 🚀 **Getting Started**

### 💻 **System Requirements**
- **RAM**: 32GB minimum (some notebooks load 18GB files)
- **Storage**: 25GB free space
- **OS**: Windows 10+, macOS 10.15+, Linux Ubuntu 18.04+

---

## 🐳 **Option 1: Docker Setup (Recommended)**

### Prerequisites
- Docker Desktop with 32GB+ memory allocation
- 25GB free disk space

### Setup Steps
1. **Configure Docker Memory**: Docker Desktop → Settings → Resources → Memory → Set to 32GB → Apply & Restart
2. **Run Container**:
   ```bash
   docker-compose up --build
   ```
3. **Access JupyterLab**: Open `http://localhost:8888`

**Note**: Data downloads automatically on first startup.

### Troubleshooting
- **Kernel Dying**: Increase Docker memory to 32GB+ in Docker Desktop settings
- **Port 8888 in use**: Change port in `docker-compose.yml`
- **Slow startup**: Normal on first run (downloads 3.9GB data)

---

## 🐍 **Option 2: Local Conda Setup**

### Prerequisites
- Python 3.10+ and Conda installed
- 32GB RAM and 25GB free space

### Setup Steps
1. **Create Environment**:
   ```bash
   conda create -n llmbot_compliance python=3.10
   conda activate llmbot_compliance
   pip install -r requirements.txt
   ```

2. **Download Data**:
   ```bash
   wget "https://drive.google.com/uc?export=download&id=16y_QrENhjra7lCDrRz7yIJqE-bwrGzXr" -O measurement_data.zip
   unzip measurement_data.zip
   ```

### Troubleshooting
- **Kernel**: Use the "llmbot_compliance" jupyter kernel. 


## 🛠 **Repository Structure**  

```plaintext
.
├── publisher_response/       # Section 4.2 "Awareness of LLM bots and the adoption of `robots.txt`"
│   ├── response_times.ipynb
│   ├── llm_release_before_after_count.ipynb
│   └── news_domains_bot_analysis.ipynb
├── bot_preference/           # Section 4.2 on "LLM bots' preferences"
│   ├── calculate_favorability.py
│   ├── favoribility_scores.ipynb
│   └── inconsistency_analysis.ipynb
├── mem_analysis/             # Section 5.1 Compliance and memorization analysis
│   ├── opensource_next_sent_gen.py
│   ├── closedsource_next_sent_gen.py
│   └── result_analysis.ipynb
├── chatgpt_casestudy/        # Section 5.2 Case Study: ChatGPT-User
│   ├── request_mydomain.py   # send a packet to server
│   └── accesslog_analysis.py
├── measurement_data/          # Extracted datasets (3.9GB)
├── requirements.txt           # Python dependencies
├── docker/                    # Docker configuration files
│   ├── Dockerfile             # Docker container configuration
│   └── .dockerignore          # Docker ignore file
├── scripts/                   # Utility scripts
│   ├── startup.sh             # Container startup script
│   └── bash_setup.sh          # Enhanced terminal configuration
├── docker-compose.yml         # Symlink to docker/docker-compose.yml (for convenience)
└── README_CCS_AE.md          # This artifact documentation
```

## 📈 **Validating Paper Claims**

### Section 4.2 Claims:
- **Table 3**: Generated by running `publisher_response/response_times.ipynb` - reproduces publisher response time analysis
- **Table 4**: Generated by running `bot_preference/inconsistency_analysis.ipynb` - analyzes inconsistencies in bot treatment across publishers

(Optinal)
- **Figure 4**: Generated by running `publisher_response/llm_release_before_after_count.ipynb` - shows robots.txt adoption trends before and after LLM release
- **Figure 5**: Generated by running `publisher_response/news_domains_bot_analysis.ipynb` - shows LLM bot inclusion changes over time for major news domains
- **Figure 6**: Generated by running `bot_preference/favoribility_scores.ipynb` - shows bot preference scores and favorability analysis
- **Figure 7**: Generated by running `bot_preference/web_policy_inconsistency.ipynb` - analyzes inconsistencies between website policy and robots.txt

### Section 5.1 Claims:
- **Table 6**: `mem_analysis/result_analysis.ipynb` summarizes the memorization analysis of LLMs using pre-computed data in the `results/` directory.

#### Optional: Generate Raw Results
If you want to regenerate the raw data files yourself, you can use the following scripts:
- `mem_analysis/opensource_next_sent_gen.py` - for open-source LLMs
- `mem_analysis/closedsource_next_sent_gen.py` - for closed-source LLMs

**Requirements for regenerating results:**

**For closed-source LLMs:** You need to configure API keys from respective LLM vendor websites.

**For open-source LLMs:** You need machines with specific GPU requirements:
- **GPT variants**: ≥8GB VRAM  
- **LLaMA & Gemma variants**: ≥40GB VRAM
- **Storage**: ≥200GB free space
- **Runtime**: ~12 hours (GPT-only) or ~1 week (full run)

📋 **Detailed setup instructions**: See `mem_analysis/mem_analysis_readme.md`


### Section 5.2 Claims:
- **Case Study**: Generated by running `chatgpt_casestudy/accesslog_analysis.py`. You can find a table related to ChatGPT-User, which summarizes the access log of ChatGPT-User. The analysis identifies that ChatGPT-User accessed the `/better/` path, which is disallowed by robots.txt, demonstrating non-compliant behavior.

