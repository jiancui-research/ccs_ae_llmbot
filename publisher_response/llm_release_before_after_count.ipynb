{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_date_str = \"20230101000000\"\n",
    "cutoff_date_str = \"20240301000000\"\n",
    "end_date_str = \"20250407000000\"\n",
    "\n",
    "start_date = datetime.strptime(start_date_str, \"%Y%m%d%H%M%S\")\n",
    "cutoff_date = datetime.strptime(cutoff_date_str, \"%Y%m%d%H%M%S\")\n",
    "num_days_expected = (cutoff_date - start_date).days+1\n",
    "\n",
    "bot2llm_date = {\n",
    "    \"Amazonbot\": \"2023-09-28\",\n",
    "    \"FacebookBot\": \"2023-02-24\",\n",
    "    \"Bytespider\": \"2023-08-18\",\n",
    "    \"Yeti\": \"2023-08-24\",\n",
    "    \"Baiduspider\": \"2023-08-31\",\n",
    "    \"PetalBot\": \"2023-03-20\",\n",
    "    \"CCBot\": \"2009-05-11\",\n",
    "    \"Omgilibot\": \"2013-12-11\",\n",
    "    \"BingBot\": \"2023-09-21\",\n",
    "}\n",
    "\n",
    "historical_robots_file = f\"../measurement_data/historical_tranco_robots_data_{start_date_str[:8]}_{end_date_str[:8]}.json\"\n",
    "with open(historical_robots_file, 'r') as f:\n",
    "    all_snapshots_data = json.load(f)\n",
    "\n",
    "csv_file = f'../measurement_data/tranco_top-1m_{end_date_str[:8]}.csv'\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    rows = list(reader)\n",
    "    total_rows = len(rows)\n",
    "url2tranco_rank = {row[1]: row[0] for row in rows}\n",
    "\n",
    "with open(f\"../measurement_data/url2botinfo_20240411.json\", 'r') as f:\n",
    "    url2botinfo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_snapshots_data = {}\n",
    "\n",
    "for site, snapshots in all_snapshots_data.items():\n",
    "    if site not in url2tranco_rank or int(url2tranco_rank[site]) > 10000:\n",
    "        continue\n",
    "    filtered_snapshots = [snapshot for snapshot in snapshots if snapshot['timestamp'] <= cutoff_date_str]\n",
    "    coverage_percentage = round((len(snapshots) / num_days_expected) * 100)\n",
    "    \n",
    "    # filter by coverage percentage\n",
    "    if filtered_snapshots and coverage_percentage >= 80:\n",
    "        filtered_snapshots_data[site] = filtered_snapshots\n",
    "\n",
    "print(f\"Total number of snapshots: {len(all_snapshots_data)}, \\n\"\n",
    "      f\"Number of filtered snapshots (top 10k sites with >=80% coverage): {len(filtered_snapshots_data)}\")\n",
    "snapshots_data = {url: data for url, data in filtered_snapshots_data.items() if url in url2botinfo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_list(robots_txt):\n",
    "    bots = []\n",
    "    for line in robots_txt.split('\\n'):\n",
    "        if line.strip().startswith('#'):\n",
    "            continue\n",
    "        parts = [part.strip() for part in line.split(':') if part.strip()]\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        \n",
    "        key, value = parts\n",
    "        key = key.lower()\n",
    "        value = value.split('#', 1)[0].strip() \n",
    "        \n",
    "        if key == 'user-agent':\n",
    "            bots.append(value)\n",
    "\n",
    "    return bots\n",
    "\n",
    "url2ts_bot_list = defaultdict(list)\n",
    "for url, snapshot_data in filtered_snapshots_data.items():\n",
    "    for snapshot in snapshot_data:\n",
    "        url2ts_bot_list[url].append(\n",
    "            (snapshot['timestamp'],\n",
    "             get_bot_list(snapshot['html_content']))\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count domains with target bots listed in first snapshot, before, and after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reinitialize unique counts to include a category for the first snapshot\n",
    "bot2count = {bot: {\"before\": set(), \"after\": set(), \"first_snapshot\": set()} for bot in bot2llm_date.keys()}\n",
    "\n",
    "# Process each website once to determine its category (before, after, or first snapshot) for each bot\n",
    "for url, snapshots_list in tqdm(snapshots_data.items()):\n",
    "    clean_url = url[4:] if url.startswith(\"www.\") else url\n",
    "    for bot, date in bot2llm_date.items():\n",
    "        bot_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "        \n",
    "        # Initialize flags to avoid redundant checks\n",
    "        added_before, added_after = False, False\n",
    "        \n",
    "        # Sort snapshots by timestamp to ensure the first snapshot is processed first\n",
    "        snapshots_list_sorted = sorted(snapshots_list, key=lambda x: x[\"timestamp\"])\n",
    "        if bot in snapshots_list_sorted[0]['html_content']:\n",
    "            bot2count[bot][\"first_snapshot\"].add(clean_url)\n",
    "\n",
    "        for snapshot in snapshots_list_sorted:\n",
    "            if not added_before or not added_after:\n",
    "                timestamp = snapshot[\"timestamp\"]\n",
    "                html_content = snapshot[\"html_content\"]\n",
    "                snapshot_date = datetime.strptime(timestamp[:8], \"%Y%m%d\")\n",
    "                if bot in html_content:\n",
    "                    # Check for the first snapshot\n",
    "                    # Then categorize based on the date relative to bot_date\n",
    "                    if snapshot_date < bot_date and not added_before:\n",
    "                        bot2count[bot][\"before\"].add(clean_url)\n",
    "                        added_before = True\n",
    "                    elif snapshot_date > bot_date and not added_after:\n",
    "                        bot2count[bot][\"after\"].add(clean_url)\n",
    "                        added_after = True\n",
    "\n",
    "# Convert sets to counts, including the first snapshot\n",
    "url_counts = {bot: {\"before\": len(urls[\"before\"]), \"after\": len(urls[\"after\"]), \"first_snapshot\": len(urls[\"first_snapshot\"])} for bot, urls in bot2count.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot (Figure 4 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming url_counts is already defined\n",
    "bots = list(url_counts.keys())\n",
    "\n",
    "# Define colors for each category\n",
    "colors = {\n",
    "    'LLM Bots': '#ffb3ba',  # Pastel red\n",
    "    'Databroker Bots': '#baffc9',  # Pastel green \n",
    "    'RAG Bots': '#c9c9ff',  # Pastel purple\n",
    "}\n",
    "\n",
    "# Define a function to assign colors based on bot names\n",
    "def get_color(bot_name):\n",
    "    if bot_name in ['CCBot', 'Omgilibot']:\n",
    "        return colors['Databroker Bots']\n",
    "    elif bot_name == 'BingBot':\n",
    "        return colors['RAG Bots']\n",
    "    else:\n",
    "        return colors['LLM Bots']\n",
    "\n",
    "# Extract counts for each bot\n",
    "before_counts = [url_counts[bot]['before'] for bot in bots]\n",
    "first_day_counts = [url_counts[bot]['first_snapshot'] for bot in bots]\n",
    "after_counts = [url_counts[bot]['after'] for bot in bots]\n",
    "\n",
    "# Define the plot size\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Define bar width and x-axis indexes\n",
    "bar_width = 0.25\n",
    "x_indexes = np.arange(len(bots))\n",
    "\n",
    "# Loop through each bot to plot the bars\n",
    "for i, bot in enumerate(bots):\n",
    "    # Get the color for the current bot\n",
    "    color = get_color(bot)\n",
    "    # print(bot, color)\n",
    "    # First Snapshot - solid fill\n",
    "    bar_first_day = ax.bar(x_indexes[i] - bar_width, first_day_counts[i], bar_width, \n",
    "                           color=color, edgecolor='black', linewidth=2)\n",
    "    ax.annotate(first_day_counts[i], (bar_first_day[0].get_x() + bar_first_day[0].get_width() / 2, \n",
    "                bar_first_day[0].get_height()), ha='center', va='bottom', fontsize=24)\n",
    "\n",
    "    # Before - apply dot pattern\n",
    "    bar_before = ax.bar(x_indexes[i], before_counts[i], bar_width, \n",
    "                        color=color, edgecolor='black', linewidth=2, hatch='\\\\\\\\')\n",
    "    \n",
    "    if bot in ['CCBot', 'Omgilibot']:\n",
    "        ax.annotate('-', (bar_before[0].get_x() + bar_before[0].get_width() / 2, \n",
    "                    bar_before[0].get_height()), ha='center', va='bottom', fontsize=24)\n",
    "    else:\n",
    "        diff_before = before_counts[i] - first_day_counts[i]\n",
    "        ax.annotate(f'+{diff_before}' if diff_before > 0 else diff_before, \n",
    "                    (bar_before[0].get_x() + bar_before[0].get_width() / 2, \n",
    "                    bar_before[0].get_height()), ha='center', va='bottom', fontsize=24)\n",
    "\n",
    "    # After - apply star pattern\n",
    "    bar_after = ax.bar(x_indexes[i] + bar_width, after_counts[i], bar_width, \n",
    "                       color=color, edgecolor='black', linewidth=2, hatch='//')\n",
    "    diff_after = after_counts[i] - before_counts[i]\n",
    "    ax.annotate(f'+{diff_after}' if diff_after > 0 else diff_after, \n",
    "                (bar_after[0].get_x() + bar_after[0].get_width() / 2, \n",
    "                bar_after[0].get_height()), ha='center', va='bottom', fontsize=24)\n",
    "\n",
    "# Set the plot's borders thickness\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "# Set labels and tick marks\n",
    "ax.set_ylabel('Number of Websites', fontsize=24)\n",
    "ax.set_xticks(x_indexes)\n",
    "ax.set_xticklabels(bots, rotation=45, ha=\"right\", fontsize=28)\n",
    "\n",
    "# Add a legend to distinguish between the categories\n",
    "ax.legend(['First Snapshot', 'Before LLM Release', 'After LLM Releases'], fontsize=28, loc='upper left')\n",
    "\n",
    "# Optimize layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/before_after_llm_release.pdf', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print domains that add bots after model release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_urls = []\n",
    "for bot, data in bot2count.items():\n",
    "    print(bot)\n",
    "    # print(data['before'] - data['after'])\n",
    "    print(data['after'] - data['before'])\n",
    "    if bot in ['Amazonbot', 'FacebookBot', 'Bytespider']:\n",
    "        new_urls += (data['after'] - data['before'])\n",
    "\n",
    "print(Counter(new_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
