{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-source LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Read the Excel file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/closedsource_sent_responses.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/__init__.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _missing_dependencies:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_missing_dependencies)\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel('results/closedsource_sent_responses.xlsx')\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered id list from google_filtered_id.txt\n",
    "with open('google_filtered_ids.txt', 'r') as file:\n",
    "    id_list = file.read().splitlines()\n",
    "\n",
    "#filter df by id_list\n",
    "df_filtered = df[df['ID'].isin(id_list)]\n",
    "df_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories (assuming you have a 'category' column in your DataFrame)\n",
    "categories = ['business & finance', 'education', 'food & drink', 'movies',\n",
    "             'music and audio', 'news and politics', 'style & fashion',\n",
    "             'television', 'video gaming']\n",
    "# categories = ['News and Politics']\n",
    "\n",
    "# Define models to analyze\n",
    "models = ['gpt-4o-mini', 'gemini-1.0-pro', \n",
    "         'claude-3-haiku-20240307', 'command-r']\n",
    "\n",
    "# Define thresholds\n",
    "thresholds = [0.83, 0.85, 0.9, 0.91, 0.95, 1.0]\n",
    "\n",
    "# Print header\n",
    "print(\"\\nROUGE-L Score Analysis by Category:\")\n",
    "print(\"=\" * 150)\n",
    "\n",
    "# Print model headers with proper spacing\n",
    "header = f\"{'Category':<25}\"\n",
    "for model in models:\n",
    "    header += f\"{model:^30}\"\n",
    "print(header)\n",
    "\n",
    "# Print threshold headers\n",
    "threshold_header = \" \" * 25\n",
    "for model in models:\n",
    "    for threshold in thresholds:\n",
    "        threshold_header += f\"{threshold:^6}\"\n",
    "print(threshold_header)\n",
    "\n",
    "print(\"-\" * 150)\n",
    "\n",
    "# Calculate and print results for each category\n",
    "overall_results = {model: {'counts': {threshold: 0 for threshold in thresholds}} for model in models}\n",
    "\n",
    "# Track unique sentences with scores above 0.85 for any model\n",
    "sentences_above_threshold = set()\n",
    "\n",
    "for category in categories:\n",
    "    category_df = df_filtered[df_filtered['category'] == category]\n",
    "    line = f\"{category:<25}\"\n",
    "    \n",
    "    for model in models:\n",
    "        rouge_scores = category_df[f'{model}_rouge_l']\n",
    "        counts = {\n",
    "            threshold: sum(1 for score in rouge_scores if score >= threshold)\n",
    "            for threshold in thresholds\n",
    "        }\n",
    "        \n",
    "        # Update overall counts\n",
    "        for threshold in thresholds:\n",
    "            overall_results[model]['counts'][threshold] += counts[threshold]\n",
    "            \n",
    "        # Track sentences above 0.85\n",
    "        sentences_above_085 = category_df[rouge_scores >= 0.85]['context_sentence']\n",
    "        sentences_above_threshold.update(sentences_above_085)\n",
    "        \n",
    "        # Format counts with proper spacing\n",
    "        for threshold in thresholds:\n",
    "            line += f\"{counts[threshold]:^6}\"\n",
    "    print(line)\n",
    "\n",
    "# Print overall results\n",
    "print(\"-\" * 150)\n",
    "line = \"Overall\".ljust(25)\n",
    "for model in models:\n",
    "    total_counts = overall_results[model]['counts']\n",
    "    for threshold in thresholds:\n",
    "        line += f\"{total_counts[threshold]:^6}\"\n",
    "print(line)\n",
    "print(\"=\" * 150)\n",
    "\n",
    "# Print total unique sentences above 0.85\n",
    "print(f\"\\nTotal unique sentences with score >= 0.85 across all models: {len(sentences_above_threshold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-source LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "def readfile(filePath):\n",
    "    lines= []\n",
    "    with open(filePath,'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def readFile(filePath):\n",
    "    lines = []\n",
    "    with open(filePath,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "def analysisTable10Result(Texts, allDatas):\n",
    "    results = {}\n",
    "    msmNegativeResult = {}\n",
    "    msmPositiveResult = {}\n",
    "    for keyText in Texts:\n",
    "        if keyText not in allDatas:\n",
    "            continue\n",
    "        if allDatas[keyText] == 0:\n",
    "            msmNegativeResult[keyText] = Texts[keyText]\n",
    "        else:\n",
    "            msmPositiveResult[keyText] = Texts[keyText]\n",
    "    r95 = 0\n",
    "    r91 = 0\n",
    "    r90 = 0\n",
    "    r85 = 0\n",
    "    r80 = 0\n",
    "    r75 = 0\n",
    "    r70 = 0\n",
    "    r65 = 0\n",
    "    r60 = 0\n",
    "    r55 = 0\n",
    "    for keyText in msmPositiveResult:\n",
    "        v = msmPositiveResult[keyText]\n",
    "        if v > 0.95:\n",
    "            r95 = r95 + 1\n",
    "        elif v >= 0.91:\n",
    "            r91 = r91 + 1\n",
    "        elif v > 0.90:\n",
    "            r90 = r90 + 1\n",
    "        elif v > 0.85:\n",
    "            r85 = r85 + 1\n",
    "        elif v > 0.80:\n",
    "            r80 = r80 + 1\n",
    "        elif v > 0.75:\n",
    "            r75 = r75 + 1\n",
    "        elif v > 0.70:\n",
    "            r70 = r70 + 1\n",
    "        elif v > 0.65:\n",
    "            r65 = r65 + 1\n",
    "            #print(keyText+':::::'+str(v))\n",
    "        elif v > 0.60:\n",
    "            r60 = r60 + 1\n",
    "            #print(keyText+':::::'+str(v))\n",
    "        else:\n",
    "            r55 = r55 + 1\n",
    "    result = []\n",
    "    result.append(r95)\n",
    "    result.append(r90)\n",
    "    result.append(r85)\n",
    "    result.append(r80)\n",
    "    result.append(r75)\n",
    "    result.append(r70)\n",
    "    result.append(r60+r65)\n",
    "    result.append(r55)\n",
    "    results['positive'] = result\n",
    "    r95 = 0\n",
    "    r91 = 0\n",
    "    r90 = 0\n",
    "    r85 = 0\n",
    "    r80 = 0\n",
    "    r75 = 0\n",
    "    r70 = 0\n",
    "    r65 = 0\n",
    "    r60 = 0\n",
    "    r55 = 0\n",
    "    for keyText in msmNegativeResult:\n",
    "        v = msmNegativeResult[keyText]\n",
    "        if v > 0.95:\n",
    "            r95 = r95 + 1\n",
    "        elif v >= 0.91:\n",
    "            r91 = r91 + 1\n",
    "        elif v > 0.90:\n",
    "            r90 = r90 + 1\n",
    "        elif v > 0.85:\n",
    "            r85 = r85 + 1\n",
    "        elif v > 0.80:\n",
    "            r80 = r80 + 1\n",
    "        elif v > 0.75:\n",
    "            r75 = r75 + 1\n",
    "        elif v > 0.70:\n",
    "            r70 = r70 + 1\n",
    "        elif v > 0.65:\n",
    "            r65 = r65 + 1\n",
    "            print(keyText)\n",
    "            print(Texts[keyText])\n",
    "        elif v > 0.60:\n",
    "            r60 = r60 + 1\n",
    "            print(keyText)\n",
    "            print(Texts[keyText])\n",
    "        else:\n",
    "            r55 = r55 + 1\n",
    "\n",
    "\n",
    "    result = []\n",
    "    result.append(r95)\n",
    "    result.append(r90)\n",
    "    result.append(r85)\n",
    "    result.append(r80)\n",
    "    result.append(r75)\n",
    "    result.append(r70)\n",
    "    result.append(r60+r65)\n",
    "    result.append(r55)\n",
    "    results['negative'] = result\n",
    "    return results\n",
    "\n",
    "def forTable10():\n",
    "    lines = []\n",
    "    allDatas = {}\n",
    "    with open('../trainDatas.txt','r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        lines = json.loads(content)\n",
    "    for line in lines:\n",
    "        allDatas[line['input']] = line['label']\n",
    "\n",
    "    # Memorization\n",
    "    msmlines = []\n",
    "    msmTexts = {}\n",
    "    msmDir = 'result/msm/'\n",
    "    msmFilenames = os.listdir(msmDir)\n",
    "    for msmFilename in msmFilenames:\n",
    "        msmFilePath = msmDir+msmFilename\n",
    "        lines = readFile(msmFilePath)\n",
    "        for line in lines:\n",
    "            items = line.split('**************')\n",
    "            words = items[1].split(' ')\n",
    "            if len(words) <= 2:\n",
    "                continue\n",
    "            if int(items[3].replace('*','')) > 62:\n",
    "                msmTexts[items[0] + '**********************' + items[1]] = float(items[4])\n",
    "    msmResults = analysisTable10Result(msmTexts, allDatas)\n",
    "\n",
    "    # SentenceBERT\n",
    "    sbertlines = []\n",
    "    sbertTexts = {}\n",
    "    sbertDir = 'result/sbert/'\n",
    "    sbertFilenames = os.listdir(sbertDir)\n",
    "    for sbertFilename in sbertFilenames:\n",
    "        sbertFilePath = sbertDir + sbertFilename\n",
    "        lines = readFile(sbertFilePath)\n",
    "        for line in lines:\n",
    "            items = line.split('**************')\n",
    "            words = items[1].split(' ')\n",
    "            if len(words) <= 2:\n",
    "                continue\n",
    "            if int(items[3].replace('*', '')) > 62:\n",
    "                sbertTexts[items[0] + '**********************' + items[1]] = float(items[4])\n",
    "    sbertResults = analysisTable10Result(sbertTexts, allDatas)\n",
    "\n",
    "    #ROUGE-L\n",
    "    rougeLlines = []\n",
    "    rougeLTexts = {}\n",
    "    rougeLDir = 'result/2023/'\n",
    "    rougeLFilenames = os.listdir(rougeLDir)\n",
    "    for rougeLFilename in rougeLFilenames:\n",
    "        rougeLFilePath = rougeLDir + rougeLFilename\n",
    "        lines = readFile(rougeLFilePath)\n",
    "        for line in lines:\n",
    "            items = line.split('**************')\n",
    "            words = items[1].split(' ')\n",
    "            if len(words) <= 2:\n",
    "                continue\n",
    "            if int(items[3].replace('*', '')) > 62:\n",
    "                rougeLTexts[items[0] + '**********************' + items[1]] = float(items[4])\n",
    "    rougeLDir = 'result/2019/'\n",
    "    rougeLFilenames = os.listdir(rougeLDir)\n",
    "    for rougeLFilename in rougeLFilenames:\n",
    "        rougeLFilePath = rougeLDir + rougeLFilename\n",
    "        lines = readFile(rougeLFilePath)\n",
    "        for line in lines:\n",
    "            items = line.split('**************')\n",
    "            words = items[1].split(' ')\n",
    "            if len(words) <= 2:\n",
    "                continue\n",
    "            if int(items[3].replace('*', '')) > 62:\n",
    "                rougeLTexts[items[0] + '**********************' + items[1]] = float(items[4])\n",
    "    rougeLResults = analysisTable10Result(rougeLTexts, allDatas)\n",
    "\n",
    "    print('TABLE 10: Comparison of similarity algorithms across LLMs in the sampled groundtruth dataset.')\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    print(' Similarity Algorithm   Dataset    <=0.6  0.6∼0.7  0.7∼0.75  0.75∼0.8  0.8∼0.85  0.85∼0.9  0.9∼0.95  0.95∼1')\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    negativeMsm = msmResults['negative']\n",
    "    print(\"                        negative   %d   %d        %d         %d         %d         %d         %d         %d\"%(negativeMsm[7], negativeMsm[6],negativeMsm[5], negativeMsm[4],negativeMsm[3], negativeMsm[2],negativeMsm[1], negativeMsm[0]))\n",
    "    print(' Memorization')\n",
    "    positiveMsm = msmResults['positive']\n",
    "    print(\"                        positive   %d   %d       %d         %d         %d         %d         %d         %d\"%(positiveMsm[7], positiveMsm[6],positiveMsm[5], positiveMsm[4],positiveMsm[3], positiveMsm[2],positiveMsm[1], positiveMsm[0]))\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    negativeSbert = sbertResults['negative']\n",
    "    print(\"                        negative   %d    %d      %d       %d       %d       %d       %d         %d\"%(negativeSbert[7], negativeSbert[6],negativeSbert[5], negativeSbert[4],negativeSbert[3], negativeSbert[2],negativeSbert[1], negativeSbert[0]))\n",
    "    print(' SentenceBERT')\n",
    "    positiveSbert = sbertResults['positive']\n",
    "    print(\"                        positive   %d    %d      %d       %d       %d       %d       %d         %d\"%(positiveSbert[7], positiveSbert[6],positiveSbert[5], positiveSbert[4],positiveSbert[3], positiveSbert[2],positiveSbert[1], positiveSbert[0]))\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    negativeRougeL = rougeLResults['negative']\n",
    "    print(\"                        negative   %d    %d       %d         %d         %d         %d        %d          %d\"%(negativeRougeL[7], negativeRougeL[6],negativeRougeL[5], negativeRougeL[4],negativeRougeL[3], negativeRougeL[2],negativeRougeL[1], negativeRougeL[0]))\n",
    "    print(' ROUGE-L')\n",
    "    positiveRougeL = rougeLResults['positive']\n",
    "    print(\"                        positive   %d    %d       %d        %d        %d        %d       %d          %d\"%(positiveRougeL[7], positiveRougeL[6],positiveRougeL[5], positiveRougeL[4],positiveRougeL[3], positiveRougeL[2],positiveRougeL[1], positiveRougeL[0]))\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "def forTable11():\n",
    "    lines = []\n",
    "    allDatas = {}\n",
    "    with open('../testDatas.txt','r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        lines = json.loads(content)\n",
    "    for line in lines:\n",
    "        allDatas[line['input']] = line['label']\n",
    "\n",
    "    msmlines = []\n",
    "    texts = {}\n",
    "    msmDir = 'result/2023/'\n",
    "    msmFilenames = os.listdir(msmDir)\n",
    "    for msmFilename in msmFilenames:\n",
    "        msmFilePath = msmDir + msmFilename\n",
    "        lines = readFile(msmFilePath)\n",
    "        for line in lines:\n",
    "            items = line.split('**************')\n",
    "            words = items[1].split(' ')\n",
    "            if len(words) <= 2:\n",
    "                continue\n",
    "            if int(items[3].replace('*', '')) > 62:\n",
    "                texts[items[0] + '**********************' + items[1]] = float(items[4])\n",
    "\n",
    "    msmDir = 'result/2019/'\n",
    "    msmFilenames = os.listdir(msmDir)\n",
    "    for msmFilename in msmFilenames:\n",
    "        msmFilePath = msmDir + msmFilename\n",
    "        lines = readFile(msmFilePath)\n",
    "        for line in lines:\n",
    "            items = line.split('**************')\n",
    "            words = items[1].split(' ')\n",
    "            if len(words) <= 2:\n",
    "                continue\n",
    "            if int(items[3].replace('*', '')) > 62:\n",
    "                texts[items[0] + '**********************' + items[1]] = float(items[4])\n",
    "\n",
    "\n",
    "    y = []\n",
    "    pred = []\n",
    "    for keyText in texts:\n",
    "        if keyText not in allDatas:\n",
    "            continue\n",
    "        if allDatas[keyText] == 0:\n",
    "\n",
    "            if texts[keyText] < 0.83:\n",
    "                y.append(0)\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                print(keyText)\n",
    "        else:\n",
    "            y.append(1)\n",
    "            if texts[keyText] >= 0.83:\n",
    "                pred.append(1)\n",
    "            else:\n",
    "                pred.append(0)\n",
    "    acc = accuracy_score(y,pred)\n",
    "    recall = recall_score(y,pred)\n",
    "    precision = precision_score(y, pred)\n",
    "    print('TABLE 11: Performance Comparison of Memorization Analysis Methods')\n",
    "    print('----------------------------------------------')\n",
    "    print('Method        Precision    Recall    Accuracy')\n",
    "    print('----------------------------------------------')\n",
    "    print('Our Method    %.4f       %.4f    %.4f'%(precision, recall, acc))\n",
    "    print('----------------------------------------------')\n",
    "def getWebsiteCategoryResult(resultDir, threshold):\n",
    "    filenames = os.listdir(resultDir)\n",
    "    allDatas = {}\n",
    "    allDatas['business'] = 0\n",
    "    allDatas['education'] = 0\n",
    "    allDatas['food'] = 0\n",
    "    allDatas['movies'] = 0\n",
    "    allDatas['music'] = 0\n",
    "    allDatas['news'] = 0\n",
    "    allDatas['fashion'] = 0\n",
    "    allDatas['television'] = 0\n",
    "    allDatas['video'] = 0\n",
    "\n",
    "    for filename in filenames:\n",
    "        lines = readfile(resultDir + filename)\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n', '')\n",
    "            items = line.split('**************')\n",
    "            if int(items[3].replace('*', '')) > 62 and float(items[4]) >= threshold:\n",
    "                if threshold < 0.9:\n",
    "                    print(filename + '********' + line)\n",
    "                if 'sportsengine.com' in filename:\n",
    "                    allDatas['business'] = allDatas['business'] + 1\n",
    "                elif 'pennlive.com' in filename:\n",
    "                    allDatas['education'] = allDatas['education'] + 1\n",
    "                elif 'silive.com' in filename:\n",
    "                    allDatas['education'] = allDatas['education'] + 1\n",
    "                elif 'epicurious.com' in filename:\n",
    "                    allDatas['food'] = allDatas['food'] + 1\n",
    "                elif 'bonappetit.com' in filename:\n",
    "                    allDatas['food'] = allDatas['food'] + 1\n",
    "                elif 'gqindia.com' in filename:\n",
    "                    allDatas['movies'] = allDatas['movies'] + 1\n",
    "                elif 'pitchfork.com' in filename:\n",
    "                    allDatas['music'] = allDatas['music'] + 1\n",
    "                elif 'nj.com' in filename:\n",
    "                    allDatas['news'] = allDatas['news'] + 1\n",
    "                elif 'al.com' in filename:\n",
    "                    allDatas['news'] = allDatas['news'] + 1\n",
    "                elif 'vogue.co.uk' in filename:\n",
    "                    allDatas['fashion'] = allDatas['fashion'] + 1\n",
    "                elif 'gq-magazine.co.uk' in filename:\n",
    "                    allDatas['fashion'] = allDatas['fashion'] + 1\n",
    "                elif 'cleveland.com' in filename:\n",
    "                    allDatas['television'] = allDatas['television'] + 1\n",
    "                elif 'gulflive.com' in filename:\n",
    "                    allDatas['video'] = allDatas['video'] + 1\n",
    "                elif 'syracuse.com' in filename:\n",
    "                    allDatas['video'] = allDatas['video'] + 1\n",
    "    allDatas['total'] = allDatas['business'] + allDatas['education'] + allDatas['food'] + allDatas['movies'] + allDatas['music'] + allDatas['news'] + allDatas['fashion'] + allDatas['television'] + allDatas['video']\n",
    "\n",
    "    return allDatas\n",
    "\n",
    "\n",
    "\n",
    "def forTable8():\n",
    "    allDatasGPT09 = getWebsiteCategoryResult('result/disallow_gpt_2019/',0.9)\n",
    "    allDatasGPT83 = getWebsiteCategoryResult('result/disallow_gpt_2019/',0.83)\n",
    "    allDatasLlama09 = getWebsiteCategoryResult('result/disallow_llama_2023/',0.9)\n",
    "    allDatasLlama83 = getWebsiteCategoryResult('result/disallow_llama_2023/',0.83)\n",
    "    allDatasGemma09 = getWebsiteCategoryResult('result/disallow_gemma_2023/',0.9)\n",
    "    allDatasGemma83 = getWebsiteCategoryResult('result/disallow_gemma_2023/',0.83)\n",
    "    print('TABLE 8: Comparison of ROUGE-L similarity scores across LLMs in disallowed categories')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('                        GPT-2-XL(1.5B)   Llama-3.1-8B    Gemma-2-9B')\n",
    "    print('Category                --------------   --------------  ---------------')\n",
    "    print('                        >=0.83  >=0.90   >=0.83  >=0.90  >=0.83  >=0.90')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Business & Finance        %d       %d        %d       %d       %d       %d'%(allDatasGPT83['business'],allDatasGPT09['business'],allDatasLlama83['business'],allDatasLlama09['business'],allDatasGemma83['business'],allDatasGemma09['business']))\n",
    "    print('Education                 %d       %d        %d       %d       %d       %d'%(allDatasGPT83['education'],allDatasGPT09['education'],allDatasLlama83['education'],allDatasLlama09['education'],allDatasGemma83['education'],allDatasGemma09['education']))\n",
    "    print('Food & Drink              %d       %d        %d       %d       %d       %d'%(allDatasGPT83['food'],allDatasGPT09['food'],allDatasLlama83['food'],allDatasLlama09['food'],allDatasGemma83['food'],allDatasGemma09['food']))\n",
    "    print('Movies                    %d       %d        %d       %d       %d       %d'%(allDatasGPT83['movies'],allDatasGPT09['movies'],allDatasLlama83['movies'],allDatasLlama09['movies'],allDatasGemma83['movies'],allDatasGemma09['movies']))\n",
    "    print('Music and Audio           %d       %d        %d       %d       %d       %d'%(allDatasGPT83['music'],allDatasGPT09['music'],allDatasLlama83['music'],allDatasLlama09['music'],allDatasGemma83['music'],allDatasGemma09['music']))\n",
    "    print('News and Politics         %d       %d        %d       %d       %d       %d'%(allDatasGPT83['news'],allDatasGPT09['news'],allDatasLlama83['news'],allDatasLlama09['news'],allDatasGemma83['news'],allDatasGemma09['news']))\n",
    "    print('Style & Fashion           %d       %d        %d       %d       %d       %d'%(allDatasGPT83['fashion'],allDatasGPT09['fashion'],allDatasLlama83['fashion'],allDatasLlama09['fashion'],allDatasGemma83['fashion'],allDatasGemma09['fashion']))\n",
    "    print('Television                %d       %d        %d       %d       %d       %d'%(allDatasGPT83['television'],allDatasGPT09['television'],allDatasLlama83['television'],allDatasLlama09['television'],allDatasGemma83['television'],allDatasGemma09['television']))\n",
    "    print('Video Gaming              %d       %d        %d       %d       %d       %d'%(allDatasGPT83['video'],allDatasGPT09['video'],allDatasLlama83['video'],allDatasLlama09['video'],allDatasGemma83['video'],allDatasGemma09['video']))\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Overall                   %d      %d        %d      %d       %d      %d'%(allDatasGPT83['total'],allDatasGPT09['total'],allDatasLlama83['total'],allDatasLlama09['total'],allDatasGemma83['total'],allDatasGemma09['total']))\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "def getSmallForGPT2019():\n",
    "    dir = 'result/disallow_gpt_2019/'\n",
    "    names = os.listdir(dir)\n",
    "    for name in names:\n",
    "        lines = readfile(dir + name)\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.replace('\\n', '')\n",
    "            items = line.split('**************')\n",
    "            if int(items[3].replace('*', '')) <= 62:\n",
    "                print(name + '********' + line)\n",
    "                items = name.split('_')\n",
    "                webname = items[4]\n",
    "                dirPath = '../finalDataForGPT/' + webname + '/'\n",
    "                filenames = os.listdir(dirPath)\n",
    "                items = line.split('**************')\n",
    "                for f in filenames:\n",
    "                    lines = readfile(dirPath + f)\n",
    "                    if items[0] in lines[1] and items[1] in lines[2]:\n",
    "                        with open('../finalDataForSmall/small62/gpt/' + webname + '_' + f, 'w', encoding='utf-8') as f:\n",
    "                            f.writelines(lines)\n",
    "\n",
    "def rewriteResult():\n",
    "    datas = {}\n",
    "    lines = readFile('../finalDataForSmall/ccnews_gpt_xl_result_small.txt')\n",
    "    for line in lines:\n",
    "        items = line.split('**************')\n",
    "        keyText = items[0]+'**************'+items[1]\n",
    "        if int(items[3]) >= 62:\n",
    "            datas[keyText] = keyText+'**************'+items[2]+'**************'+items[3]+'**************'+items[4]+'\\n'\n",
    "    dirPath = 'result/disallow_gpt_2019/'\n",
    "    fileNames = os.listdir(dirPath)\n",
    "    for fileName in fileNames:\n",
    "        newlines = []\n",
    "        filePath = dirPath+fileName\n",
    "        lines = readFile(filePath)\n",
    "        for line in lines:\n",
    "            items = line.split('**************')\n",
    "            keyText = items[0]+'**************'+items[1]\n",
    "            if keyText in datas:\n",
    "                newlines.append(datas[keyText])\n",
    "                datas.pop(keyText)\n",
    "            else:\n",
    "                newlines.append(line)\n",
    "        with open(filePath,'w',encoding='utf-8') as f:\n",
    "            for line in newlines:\n",
    "                f.write(line)\n",
    "\n",
    "\n",
    "#getSmallForGPT2019()\n",
    "#rewriteResult()\n",
    "forTable8()\n",
    "forTable10()\n",
    "forTable11()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
